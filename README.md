ğŸ§ Tajweed Rule Classification â€” ConvNeXt + Log-Mel Spectrograms

End-to-end audio classification pipeline for recognizing Tajweed rules from Quran recitation audio clips using log-Mel spectrograms and ConvNeXt. Developed as part of the KAUST vs KKU ML Tournament â€” Round 5.

â¸»

ğŸ“˜ Overview

This project builds a multi-class classifier to identify the Tajweed rule applied in a short audio clip of Quran recitation.
	â€¢	Input: .wav files sampled at 16kHz
	â€¢	Output: One of 4 Tajweed rule labels (e.g., â€œIkhfaâ€, â€œIdghamâ€)
	â€¢	Model: ConvNeXt-Small pretrained on ImageNet, fine-tuned on log-Mel spectrograms
	â€¢	Cross-validation: 5-fold StratifiedKFold with macro-F1 metric

â¸»

ğŸ“Š Dataset
	â€¢	Train CSV: train.csv â€” contains id, label_name, and sheikh_name
	â€¢	Test CSV: test.csv â€” contains id, sheikh_name (no labels)
	â€¢	Audio: train/ and test/ folders with .wav files

Each audio file corresponds to a short recitation clip. Class balance and reciter diversity are visualized during EDA.

â¸»

ğŸ” EDA Highlights
	â€¢	Label distribution and reciter balance between train and test
	â€¢	Duration histogram: Most audios ~3â€“6 seconds
	â€¢	RMS and Energy plots show dynamic range of clips
	â€¢	Leaked samples removed by hashing waveforms
	â€¢	Very short audios (<0.5s) removed from train set

â¸»

ğŸ›ï¸ Configuration
	â€¢	SAMPLE_RATE = 16000
	â€¢	MAX_SEC = 6.0
	â€¢	MAX_LEN = 96000 samples
	â€¢	BATCH = 64, EPOCHS = 10, LR = 3e-4
	â€¢	K_FOLDS = 5
	â€¢	DEVICE = 'cuda' if available

â¸»

ğŸ§ª Input Pipeline: Log-Mel Spectrograms

Custom Dataset class handles:
	â€¢	Resampling to 16kHz
	â€¢	Padding or trimming to fixed duration
	â€¢	Computing MelSpectrogram â†’ log-Mel (dB)
	â€¢	Normalization using global mean/std
	â€¢	Repeating to 3 channels for image models

ğŸ›ï¸ Augmentations include Gaussian noise, time-stretching, random gain, SpecAugment (Time/Frequency masking)

â¸»

ğŸ—ï¸ Model: ConvNeXt-Small

from torchvision.models import convnext_small, ConvNeXt_Small_Weights
model = convnext_small(weights=ConvNeXt_Small_Weights.IMAGENET1K_V1)
model.classifier[2] = nn.Linear(in_features, n_classes)

	â€¢	Final classifier head is swapped to match n_classes=4
	â€¢	Pretrained backbone fine-tuned end-to-end

â¸»

ğŸ§  Loss Function: Focal Loss

To handle class imbalance, we use Focal Loss with label smoothing:

FocalLoss(gamma=2.0, smoothing=0.1, weight=class_weights)

Class weights are computed from the training labels.

â¸»

ğŸ” Training Procedure
	â€¢	5-fold StratifiedKFold on label_name
	â€¢	Each fold trains for 10 epochs
	â€¢	Model checkpoint (.pt) saved per fold based on best val_F1
	â€¢	Metrics: Macro F1 Score (torchmetrics)

Example results:

Fold1 best val_F1 = 0.9430
Fold2 best val_F1 = 0.9535
Fold3 best val_F1 = 0.9440
Fold4 best val_F1 = 0.9510
Fold5 best val_F1 = 0.9717
â†’ Mean CV F1 = 0.9527


â¸»

ğŸ” Inference & Submission
	â€¢	Load all 5 fold checkpoints
	â€¢	Apply Test-Time Augmentation (TTA): 5 rounds of SpecAugment
	â€¢	Average softmax predictions across folds & TTA rounds
	â€¢	Decode label_id â†’ label_name using LabelEncoder
	â€¢	Save submission.csv

â¸»

ğŸ“Š Test Distribution Visualization

Final predicted class distribution on test set is plotted to inspect balance and model bias.

â¸»

âœ… Summary

This solution achieved 95.2% macro-F1 average across 5 folds using:
	â€¢	Robust log-Mel spectrogram preprocessing
	â€¢	ConvNeXt-Small pretrained backbone
	â€¢	Focal loss with label smoothing and class weights
	â€¢	5-fold stratified CV + TTA during inference

ğŸ† Strong baseline with room for improvements like:
	â€¢	Using larger models (e.g., ConvNeXt-Base, Swin)
	â€¢	Pseudo-labeling test data
	â€¢	Advanced audio augmentations (SpecMix, MixUp)

â¸»

ğŸ“ Deliverables:
	â€¢	effb0_fold{1â€“5}.pt (checkpoints)
	â€¢	submission.csv
	â€¢	Optional: training logs & sample predictions
